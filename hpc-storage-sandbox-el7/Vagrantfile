# vi: set ft=ruby :
Vagrant.configure("2") do |config|

	# Use box from Manager For Lustre project - this differs from the
	# "official" CentOS base box by using a SATA controller instead of IDE.
	# This simplifies the addition of extra disks for the storage servers.
	config.vm.box = "manager-for-lustre/centos76-1810-base"

	# Set the default RAM allocation for each VM.
	# 1GB is sufficient for demo and training purposes.
	# Admin server is allocated 2GB RAM.
	# Refer to the VM definition to change.
	config.vm.provider "virtualbox" do |vbx|
		vbx.memory = 1024
	end

	# Directory root for additional vdisks for MGT, MDT0, and OSTs
	vdisk_root = "#{ENV['HOME']}/VirtualBox\ VMs/vdisks"
	# Number of shared disk devices per OSS server pair
	sdnum=8

	# Create a set of /24 networks under a single /16 subnet range
	subnet_prefix="10.73"
	# Management network for admin comms
	mgmt_net_pfx="#{subnet_prefix}.10"
	# Lustre / HPC network
	lnet_pfx="#{subnet_prefix}.20"
	# Subnet index used to create cross-over nets for each HA cluster pair
	xnet_idx=230

	# Create a basic hosts file for the VMs.
	open('hosts', 'w') { |f|
	f.puts <<-__EOF
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6

#{mgmt_net_pfx}.9 b.lfs.local b
#{mgmt_net_pfx}.10 adm.lfs.local adm
#{mgmt_net_pfx}.11 mds1.lfs.local mds1
#{mgmt_net_pfx}.12 mds2.lfs.local mds2
#{mgmt_net_pfx}.21 oss1.lfs.local oss1
#{mgmt_net_pfx}.22 oss2.lfs.local oss2
#{mgmt_net_pfx}.23 oss3.lfs.local oss3
#{mgmt_net_pfx}.24 oss4.lfs.local oss4
__EOF
	(1..8).each do |cidx|
		f.puts "#{mgmt_net_pfx}.3#{cidx} c#{cidx}.lfs.local c#{cidx}\n"
	end
	}
	config.vm.provision "shell", inline: "cp -f /vagrant/hosts /etc/hosts"
	config.vm.provision "shell", inline: "selinuxenabled && setenforce 0; cat >/etc/selinux/config<<__EOF
SELINUX=disabled
SELINUXTYPE=targeted
__EOF"

	# A simple way to create a key that can be used to enable
	# SSH between the virtual guests.
	#
	# The private key is copied onto the root account of the
	# administration node and the public key is appended to the
	# authorized_keys file of the root account for all nodes
	# in the cluster.
	#
	# Shelling out may not be the most Vagrant-friendly means to
	# create this key but it avoids more complex methods such as
	# developing a plugin.
	#
	# Popen may be a more secure way to exec but is more code
	# for what is, in this case, a relatively small gain.
	if not(File.exist?("id_rsa"))
		res = system("ssh-keygen -t rsa -N '' -f id_rsa")
	end

	# Add the generated SSH public key to each host's
	# authorized_keys file.
	config.vm.provision "shell", inline: "mkdir -m 0700 -p /root/.ssh; [ -f /vagrant/id_rsa.pub ] && (awk -v pk=\"`cat /vagrant/id_rsa.pub`\" 'BEGIN{split(pk,s,\" \")} $2 == s[2] {m=1;exit}END{if (m==0)print pk}' /root/.ssh/authorized_keys )>> /root/.ssh/authorized_keys; chmod 0600 /root/.ssh/authorized_keys"


	#
	# Create an admin server for the cluster
	#
	config.vm.define "adm", primary: true do |adm|
		adm.vm.provider "virtualbox" do |v|
			v.memory = 2048
			v.cpus = 4
			v.name = "adm"
		end
		adm.vm.host_name = "adm.lfs.local"
		adm.vm.network "forwarded_port", guest: 443, host: 8443
		# Admin / management network
		adm.vm.network "private_network",
			ip: "#{mgmt_net_pfx}.10",
			netmask: "255.255.255.0"
		adm.vm.provision "shell", inline: "mkdir -m 0700 -p /root/.ssh; cp /vagrant/id_rsa /root/.ssh/.; chmod 0600 /root/.ssh/id_rsa"

		adm.vm.provision 'install-iml-locks-and-testing',
		type: 'shell', run: 'never',
		inline: <<-SHELL
		 yum-config-manager --add-repo=https://copr.fedorainfracloud.org/coprs/managerforlustre/iml-locks-and-actions-devel/repo/epel-7/managerforlustre-iml-locks-and-actions-devel-epel-7.repo
		 yum-clean-all
		 yum install -y iml-gui
		 yum install -y iml-old-gui
		 yum install -y rust-iml-warp-drive
		 yum install -y iml-wasm-components
		 cp /vagrant/iml-4.0.10.tar.gz /tmp
		 cd /tmp
		 tar xvf iml-4.0.10.tar.gz
		SHELL
	end

	#
	# Create the metadata servers (HA pair)
	#
	(1..2).each do |mds_idx|
		config.vm.define "mds#{mds_idx}" do |mds|
			# Create additional storage to be shared between
			# the metadata server VMs.
			# Storage services associated with these #{vdisk_root}
			# will be maintained using HA failover.
			mds.vm.provider "virtualbox" do |vbx|
				vbx.name = "mds#{mds_idx}"
				vbx.cpus = 4

				if mds_idx==1 && not(File.exist?("#{vdisk_root}/mgs.vdi"))
					vbx.customize ["createmedium", "disk",
						"--filename", "#{vdisk_root}/mgs.vdi",
						"--size", "512",
						"--format", "VDI",
						"--variant", "fixed"]

				end
				if mds_idx==1 && not(File.exist?("#{vdisk_root}/mdt0.vdi"))
					vbx.customize ["createmedium", "disk",
						"--filename", "#{vdisk_root}/mdt0.vdi",
						"--size", "5120",
						"--format", "VDI",
						"--variant", "fixed"]
				end

				# Additional storage for MGS and MDT0
				# Attached to internal controller that is part
				# of HPDD base box. If using a different box
				# then may need to add SATA controller.
				vbx.customize ["storageattach", :id,
					"--storagectl", "SATA Controller",
					"--port", "1",
					"--type", "hdd",
					"--medium", "#{vdisk_root}/mgs.vdi",
					"--mtype", "shareable",
					"--device", "0"]
				vbx.customize [
					'setextradata', :id,
					'VBoxInternal/Devices/ahci/0/Config/Port1/SerialNumber',
					'MGS00000000000000000'
				]
				vbx.customize ["storageattach", :id,
					"--storagectl", "SATA Controller",
					"--port", "2",
					"--type", "hdd",
					"--medium", "#{vdisk_root}/mdt0.vdi",
					"--mtype", "shareable",
					"--device", "0"]
				vbx.customize [
					'setextradata', :id,
					'VBoxInternal/Devices/ahci/0/Config/Port2/SerialNumber',
					'MDT00000000000000000'
				]
			end
			# Set host name of VM
			mds.vm.host_name = "mds#{mds_idx}.lfs.local"
			# Admin / management network
			mds.vm.network "private_network",
				ip: "#{mgmt_net_pfx}.1#{mds_idx}",
				netmask: "255.255.255.0"
			# Lustre / application network
			mds.vm.network "private_network",
				ip: "#{lnet_pfx}.1#{mds_idx}",
				netmask: "255.255.255.0"
			# Private network to simulate crossover.
			# Used exclusively as additional cluster network
			mds.vm.network "private_network",
				ip: "#{subnet_prefix}.#{xnet_idx}.1#{mds_idx}",
				netmask: "255.255.255.0",
				auto_config: false

			mds.vm.provision 'fix-repo-file',
				type: 'shell', run: 'never',
				inline: <<-SHELL
				sed -i /proxy=_none_/d /etc/yum.repos.d/Intel-Lustre-Agent.repo
			SHELL

			# Increment the "crossover" subnet number so that
			# each HA pair has a unique "crossover" subnet
			if mds_idx % 2 == 0
				xnet_idx+=1
			end
		end
	end

	#
	# Create the object storage servers (OSS)
	# Servers are configured in HA pairs
	# By default, only the first 2 nodes are created
	# To instantiate oss3 and oss4, use this command:
	# 	vagrant up oss{3,4}
	#
	(1..4).each do |oss_idx|
		config.vm.define "oss#{oss_idx}",
			autostart: (oss_idx>2 ? false : true) do |oss|

			# Create additional storage to be shared between
			# the object storage server VMs.
			# Storage services associated with these #{vdisk_root}
			# will be maintained using HA failover.
			oss.vm.provider "virtualbox" do |vbx|
				vbx.name = "oss#{oss_idx}"
				vbx.cpus = 4

				# Set the OST index range based on the node number.
				# Each OSS is one of a pair, and will share these devices
				# Equation assumes that OSSs are allocated in pairs with
				# consecutive numbering. Each pair of servers has a set
				# of shared virtual disks (vdisks) numbered in the range
				# osd_min to osd_max. e.g.:
				# oss{1,2} share OST0..OST7 and oss{3,4} share OST8..OST16,
				# assuming the number of disks per pair (sdnum) is 8
				osd_min = ((oss_idx-1) / 2) * sdnum
				osd_max = osd_min + 7
				# Create the virtual disks for the OSTs
				# Only create the vdisks on odd-numbered VMs
				# (node 1 in each HA pair)
				if oss_idx % 2 == 1
					(osd_min..osd_max).each do |ost|
						if not(File.exist?("#{vdisk_root}/ost#{ost}.vdi"))
							vbx.customize ["createmedium", "disk",
								"--filename", "#{vdisk_root}/ost#{ost}.vdi",
								"--size", "5120",
								"--format", "VDI",
								"--variant", "fixed"
								]
						end
					end
				end

				# Attach the vdisks to each OSS in the pair
				(osd_min..osd_max).each do |osd|
					pnum = (osd % sdnum) +1
					vbx.customize ["storageattach", :id,
						"--storagectl", "SATA Controller",
						"--port", "#{pnum}",
						"--type", "hdd",
						"--medium", "#{vdisk_root}/ost#{osd}.vdi",
						"--mtype", "shareable",
						"--comment","OST%04d" % [osd]
						]
					vbx.customize [
						'setextradata', :id,
						"VBoxInternal/Devices/ahci/0/Config/Port#{pnum}/SerialNumber",
						"OST#{osd}PORT#{pnum}".ljust(20, '0')
					]
				end
			end

			oss.vm.host_name = "oss#{oss_idx}.lfs.local"
			# Admin / management network
			oss.vm.network "private_network",
				ip: "#{mgmt_net_pfx}.2#{oss_idx}",
				netmask: "255.255.255.0"
			# Lustre / application network
			oss.vm.network "private_network",
				ip: "#{lnet_pfx}.2#{oss_idx}",
				netmask: "255.255.255.0"
			# Private network to simulate crossover.
			# Used exclusively as additional cluster network
			oss.vm.network "private_network",
				ip: "#{subnet_prefix}.#{xnet_idx}.2#{oss_idx}",
				netmask: "255.255.255.0",
				auto_config: false

			oss.vm.provision 'fix-repo-file',
				type: 'shell', run: 'never',
				inline: <<-SHELL
				sed -i /proxy=_none_/d /etc/yum.repos.d/Intel-Lustre-Agent.repo
			SHELL

			# Increment the "crossover" subnet number so that
			# each HA pair has a unique "crossover" subnet
			if oss_idx % 2 == 0
				xnet_idx+=1
			end
		end
	end

	# Create a set of compute nodes.
	# By default, only 2 compute nodes are created.
	# The configuration supports a maximum of 8 compute nodes.
	(1..8).each do |c_idx|
		config.vm.define "c#{c_idx}",
			autostart: (c_idx>2 ? false : true) do |c|
			c.vm.host_name = "c#{c_idx}.lfs.local"
			# Admin / management network
			c.vm.network "private_network",
				ip: "#{mgmt_net_pfx}.3#{c_idx}",
				netmask: "255.255.255.0"
			# Lustre / application network
			c.vm.network "private_network",
				ip: "#{lnet_pfx}.3#{c_idx}",
				netmask: "255.255.255.0"

			c.vm.provider "virtualbox" do |v|
				v.name = "c#{c_idx}"
			end
		end
	end
end
